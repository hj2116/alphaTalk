{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSrpqxJNmDdI"
      },
      "source": [
        "**네이버 토론방**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KjRbxNePcJn",
        "outputId": "ae1281ae-5501-4785-bc4f-c79ad2476c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "종목 코드를 입력하세요: ): 005930\n",
            "CSV 파일로 저장 완료: naver_discussion_005930.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from urllib.parse import parse_qs, urlparse\n",
        "import pandas as pd\n",
        "\n",
        "# User-Agent 설정\n",
        "headers_pc = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/114.0.0.0 Safari/537.36\"\n",
        "}\n",
        "headers_mobile = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) \"\n",
        "                  \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1\"\n",
        "}\n",
        "\n",
        "def fetch_posts(stock_code, max_page=250):\n",
        "    data = []\n",
        "    for page in range(1, max_page + 1):\n",
        "        url = f\"https://finance.naver.com/item/board.naver?code={stock_code}&page={page}\"\n",
        "        resp = requests.get(url, headers=headers_pc)\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        post_links = soup.select(\"td.title a\")\n",
        "\n",
        "        for post_link in post_links:\n",
        "            title = post_link.text.strip()\n",
        "            relative_href = post_link[\"href\"]\n",
        "            qs = parse_qs(urlparse(relative_href).query)\n",
        "            nid = qs.get(\"nid\", [None])[0]\n",
        "            if not nid:\n",
        "                continue\n",
        "\n",
        "            iframe_src = f\"https://m.stock.naver.com/pc/domestic/stock/{stock_code}/discussion/{nid}\"\n",
        "            try:\n",
        "                resp2 = requests.get(iframe_src, headers=headers_mobile)\n",
        "                soup2 = BeautifulSoup(resp2.text, \"html.parser\")\n",
        "                script_tag = soup2.find(\"script\", id=\"__NEXT_DATA__\")\n",
        "                if not script_tag:\n",
        "                    continue\n",
        "                data_json = json.loads(script_tag.string)\n",
        "                content_html = data_json[\"props\"][\"pageProps\"][\"dehydratedState\"][\"queries\"][1][\"state\"][\"data\"][\"result\"][\"contentHtml\"]\n",
        "                content_soup = BeautifulSoup(content_html, \"html.parser\")\n",
        "                text = content_soup.get_text(separator=\"\\n\").strip()\n",
        "                data.append({\"제목\": title, \"본문\": text})\n",
        "            except:\n",
        "                continue\n",
        "    return data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    stock_code = input(\"종목 코드를 입력하세요: \").strip()\n",
        "    posts = fetch_posts(stock_code, max_page=50) #페이지 단위로 추출\n",
        "    df = pd.DataFrame(posts)\n",
        "    output_file = f\"naver_discussion_{stock_code}.csv\"\n",
        "    df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"CSV 파일로 저장 완료: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6h5eTg4mJXz"
      },
      "source": [
        "**네이버 증권 뉴스**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY-5wQrSSpBo",
        "outputId": "fc536fc4-ad6f-419e-9592-6921d887eec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "종목 코드를 입력하세요: 005930\n",
            "naver_news_005930.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# KoBERT 모델 로드\n",
        "MODEL_NAME = \"snunlp/KR-FinBERT-SC\"\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "model.eval()\n",
        "\n",
        "def fetch_news(stock_code, max_pages=10):\n",
        "    all_news = []\n",
        "    for page in range(1, max_pages + 1):\n",
        "        url = f\"https://finance.naver.com/item/news_news.naver?code={stock_code}&page={page}&sm=title_entity_id.basic&clusterId=\"\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0\",\n",
        "            \"Referer\": \"https://finance.naver.com/\"\n",
        "        }\n",
        "        resp = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        rows = soup.select(\"table.type5 tr\")\n",
        "        for row in rows:\n",
        "            a_tag = row.select_one(\"td.title a\")\n",
        "            if a_tag:\n",
        "                title = a_tag.get_text(strip=True)\n",
        "                link = a_tag[\"href\"]\n",
        "                press = row.select_one(\"td.info\")\n",
        "                press_text = press.get_text(strip=True) if press else \"\"\n",
        "                date = row.select_one(\"td.date\")\n",
        "                date_text = date.get_text(strip=True) if date else \"\"\n",
        "\n",
        "                full_link = \"https://finance.naver.com\" + link\n",
        "                all_news.append({\n",
        "                    \"날짜\": date_text,\n",
        "                    \"언론사\": press_text,\n",
        "                    \"제목\": title,\n",
        "                    \"링크\": full_link\n",
        "                })\n",
        "    return all_news\n",
        "\n",
        "def fetch_article_content(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0\",\n",
        "        \"Referer\": \"https://finance.naver.com\"\n",
        "    }\n",
        "    resp = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    # 리다이렉트 처리\n",
        "    redirect_script = soup.select_one(\"script\")\n",
        "    if redirect_script and \"top.location.href\" in redirect_script.get_text():\n",
        "        try:\n",
        "            redirect_url = redirect_script.get_text().split(\"top.location.href='\")[1].split(\"'\")[0]\n",
        "            resp = requests.get(redirect_url, headers=headers)\n",
        "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        except Exception:\n",
        "            return \"리다이렉트 실패\"\n",
        "\n",
        "    content_div = soup.select_one(\"#dic_area\")\n",
        "    if content_div:\n",
        "        return content_div.get_text(separator=\"\\n\").strip()\n",
        "    else:\n",
        "        return \"None\"\n",
        "\n",
        "def compute_sentiment_score(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1).squeeze()\n",
        "    score = -1 * probs[0].item() + 0 * probs[1].item() + 1 * probs[2].item()\n",
        "    return round(score, 4)\n",
        "\n",
        "def save_news_with_sentiment(stock_code, max_pages=10, filename=f\"naver_news_{stock_code}.csv\"):\n",
        "    news_list = fetch_news(stock_code, max_pages)\n",
        "    for news in news_list:\n",
        "        content = fetch_article_content(news[\"링크\"])\n",
        "        news[\"본문\"] = content if content else \"본문 없음\"\n",
        "        news[\"감성점수\"] = compute_sentiment_score(content) if content else 0.0\n",
        "        news.pop(\"언론사\", None)\n",
        "        news.pop(\"링크\", None)\n",
        "\n",
        "    fieldnames = [\"날짜\", \"제목\", \"본문\", \"감성점수\"]\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for item in news_list:\n",
        "            writer.writerow(item)\n",
        "\n",
        "    print(f\"{filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    stock_code = input(\"종목 코드를 입력하세요: \").strip()\n",
        "    save_news_with_sentiment(stock_code, max_pages=5) #페이지 단위로 추출\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7-dKORsw4qJ",
        "outputId": "58cab563-d770-47e9-b732-1ee24b57af08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.2.1)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.4.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.7.14)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=b8e9ef3f56c2495aa54d9663082b142b5b9dee109e34361c27db6cb011b544f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=870cf85271e6064d0c470d0c5eaa35227b4b29f4e60c0101bb547a28a7925cd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=6bf96805b182563adbc8183314d0432612e25d3022a75a2f5dd7f72ccdcb703f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=4a17a23f01403478572df8068c318bc03f80f63a01e2bb8a8494dc49bf9ded27\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.0\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Requirement already satisfied: lxml[html_clean] in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.2.1)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
            "Collecting lxml_html_clean (from lxml[html_clean])\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.7.14)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install newspaper3k\n",
        "!pip install newspaper3k lxml[html_clean]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C-0uTcRu7KxE",
        "outputId": "e9b026e4-d84a-4b9e-bdae-0ad0345a675f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "0dd9503ab9db4196a9410ed0c81cf5b4",
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install transformers torch\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmIWMPsamPkU"
      },
      "source": [
        "**newsapi 해외뉴스**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1lKosLB2Oe4",
        "outputId": "cf6b9a87-43c4-4ca2-cfec-7d7d0fe64e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "티커를 입력하세요: AAPL\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "newsapi_AAPL.csv 총 45건\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from newspaper import Article\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# FinBERT 모델 초기화\n",
        "def load_finbert_model():\n",
        "    model_name = \"yiyanghkust/finbert-tone\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "    return tokenizer, model\n",
        "\n",
        "# 텍스트 정제 함수\n",
        "def clean_text(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = text.lower()  # 소문자화\n",
        "    text = re.sub(r'\\s+', ' ', text)  # 다중 공백 제거\n",
        "    text = re.sub(r'\\n+', ' ', text)  # 줄바꿈 제거\n",
        "    text = re.sub(r'[^a-zA-Z0-9.,!?\\'\\\"%$()-]', ' ', text)  # 특수문자 제거\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# 감성 점수 계산 함수\n",
        "#softmax: 각 클래스의 확률로 변환 (값의 총합 = 1)\n",
        "\n",
        "def compute_sentiment_finbert(text, tokenizer, model):\n",
        "    try:\n",
        "        text = clean_text(text)\n",
        "        tokens = tokenizer.encode(text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "        with torch.no_grad():\n",
        "            output = model(tokens)\n",
        "        logits = output.logits\n",
        "        probs = torch.softmax(logits, dim=1).numpy()[0]\n",
        "        sentiment_score = probs[2] - probs[0]\n",
        "        return round(float(sentiment_score), 4)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# 뉴스 수집 및 본문 크롤링 함수\n",
        "def fetch_news_with_full_body_and_sentiment(ticker, days=3, max_results=100, api_key=\"5a9d9ae903084a349d94fdff1fb3da6a\"):\n",
        "    url = \"https://newsapi.org/v2/everything\"\n",
        "    end_date = datetime.utcnow()\n",
        "    start_date = end_date - timedelta(days=days)\n",
        "    from_date_str = start_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "    all_articles = []\n",
        "    page = 1\n",
        "\n",
        "    while True:\n",
        "        params = {\n",
        "            \"q\": ticker,\n",
        "            \"from\": from_date_str,\n",
        "            \"language\": \"en\",\n",
        "            \"sortBy\": \"publishedAt\",\n",
        "            \"pageSize\": 100,\n",
        "            \"page\": page,\n",
        "            \"apiKey\": api_key\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "        data = response.json()\n",
        "\n",
        "        if data.get(\"status\") != \"ok\":\n",
        "            print(f\"Error: {data.get('message')}\")\n",
        "            break\n",
        "\n",
        "        articles = data.get(\"articles\", [])\n",
        "        if not articles:\n",
        "            break\n",
        "\n",
        "        all_articles.extend(articles)\n",
        "\n",
        "        if len(all_articles) >= max_results or len(articles) < 100:\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "\n",
        "    # 본문 크롤링 및 감성 점수 적용\n",
        "    tokenizer, model = load_finbert_model()\n",
        "\n",
        "    for article in all_articles:\n",
        "        url = article.get(\"url\")\n",
        "        full_text = \"\"\n",
        "        if url:\n",
        "            try:\n",
        "                news_article = Article(url)\n",
        "                news_article.download()\n",
        "                news_article.parse()\n",
        "                full_text = news_article.text\n",
        "            except Exception:\n",
        "                full_text = article.get(\"description\") or \"\"\n",
        "        else:\n",
        "            full_text = article.get(\"description\") or \"\"\n",
        "\n",
        "        article[\"full_body\"] = full_text\n",
        "        article[\"sentiment_score\"] = compute_sentiment_finbert(full_text, tokenizer, model)\n",
        "\n",
        "    newsapi = pd.DataFrame(all_articles)\n",
        "    newsapi = newsapi[[\"publishedAt\", \"title\", \"full_body\", \"sentiment_score\"]]\n",
        "    newsapi[\"publishedAt\"] = pd.to_datetime(newsapi[\"publishedAt\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    newsapi.rename(columns={\n",
        "        \"publishedAt\": \"Date\",\n",
        "        \"title\": \"Title\",\n",
        "        \"full_body\": \"Body\",\n",
        "        \"sentiment_score\": \"SentimentScore\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    newsapi = newsapi.head(max_results)\n",
        "\n",
        "    start_str = start_date.strftime(\"%Y%m%d\")\n",
        "    end_str = end_date.strftime(\"%Y%m%d\")\n",
        "    filename = f\"newsapi_{ticker}.csv\"\n",
        "    newsapi.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"{filename} 총 {len(newsapi)}건\")\n",
        "\n",
        "    return newsapi\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ticker_input = input(\"티커를 입력하세요: \").strip()\n",
        "    fetch_news_with_full_body_and_sentiment(ticker_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIat2JSBTf54",
        "outputId": "bca1bd30-5637-4b43-d370-6ae02fedb64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: asyncpraw in /usr/local/lib/python3.11/dist-packages (7.8.1)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (24.1.0)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite<=0.17.0 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (0.17.0)\n",
            "Requirement already satisfied: asyncprawcore<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (0.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (1.20.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.14.1)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from update_checker>=0.18->asyncpraw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (2025.7.14)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install asyncpraw\n",
        "!pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nru-0PdImTmD"
      },
      "source": [
        "**레딧** <br>\n",
        "실행 후 티커가 아닌 검색어 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhhpvvGFfaIY",
        "outputId": "352a67d1-1400-461c-d767-21612f332993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "검색어를 입력하세요: Apple\n",
            "'reddit_Apple.csv' 총 13건 \n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "import asyncpraw\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "reddit_api = asyncpraw.Reddit(\n",
        "    client_id=\"4KYCTTNnmyNxyQwDS_fy6g\",\n",
        "    client_secret=\"CvxFEpg_IDU_uYxjJsrQWjOR2eXpnA\",\n",
        "    user_agent=\"stock_posts by /u/Leather_Arugula7011\"\n",
        ")\n",
        "\n",
        "async def fetch_reddit_posts_async(ticker, subreddits=[\"stocks\", \"wallstreetbets\", \"investing\"], days=3, limit=100):\n",
        "    results = []\n",
        "    now = datetime.utcnow()\n",
        "    threshold_date = (now - timedelta(days=days)).date()\n",
        "    time_threshold = datetime.combine(threshold_date, datetime.min.time())\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        subreddit = await reddit_api.subreddit(subreddit_name)\n",
        "        async for submission in subreddit.search(ticker, sort=\"new\", limit=limit):\n",
        "            post_date = datetime.utcfromtimestamp(submission.created_utc)\n",
        "            if post_date < time_threshold:\n",
        "                continue\n",
        "\n",
        "            results.append({\n",
        "                \"Subreddit\": subreddit_name,\n",
        "                \"Date\": post_date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"Title\": submission.title,\n",
        "                \"Body\": submission.selftext[:500],\n",
        "                \"Link\": f\"https://www.reddit.com{submission.permalink}\"\n",
        "            })\n",
        "\n",
        "    reddit_df = pd.DataFrame(results)\n",
        "    filename = f\"reddit_{ticker}.csv\"\n",
        "    reddit_df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"'{filename}' 총 {len(reddit_df)}건 \")\n",
        "\n",
        "# Jupyter 환경에서 실행 예시\n",
        "ticker = input(\"검색어를 입력하세요: \").strip()\n",
        "await fetch_reddit_posts_async(ticker)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
