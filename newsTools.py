import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import re
from dotenv import load_dotenv
import os
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import yfinance as yf
import feedparser
from urllib.parse import quote
import time

load_dotenv()

class NewsAnalyzer:
    def __init__(self):
        # FinBERT ëª¨ë¸ ë¡œë“œ (ê¸ˆìœµ íŠ¹í™” ê°ì • ë¶„ì„)
        try:
            self.finbert_tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
            self.finbert_model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
            self.sentiment_pipeline = pipeline("sentiment-analysis", 
                                             model=self.finbert_model, 
                                             tokenizer=self.finbert_tokenizer)
            print("FinBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"FinBERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.sentiment_pipeline = None
    
    def scrape_naver_news(self, ticker, company_name, days=7):
        """ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ê¸°ì—… ê´€ë ¨ ë‰´ìŠ¤ ìŠ¤í¬ëž˜í•‘"""
        try:
            news_data = []
            
            # ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •
            search_terms = [company_name, ticker.replace('.KS', '')]
            
            for term in search_terms:
                encoded_term = quote(term)
                url = f"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={encoded_term}"
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                
                response = requests.get(url, headers=headers)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ë‰´ìŠ¤ ê¸°ì‚¬ ì¶”ì¶œ
                news_items = soup.find_all('div', class_='news_area')
                
                for item in news_items[:10]:  # ìƒìœ„ 10ê°œë§Œ
                    try:
                        title_elem = item.find('a', class_='news_tit')
                        if title_elem:
                            title = title_elem.get_text().strip()
                            link = title_elem.get('href')
                            
                            # ìš”ì•½ë¬¸ ì¶”ì¶œ
                            summary_elem = item.find('div', class_='news_dsc')
                            summary = summary_elem.get_text().strip() if summary_elem else ""
                            
                            # ë‚ ì§œ ì¶”ì¶œ
                            date_elem = item.find('span', class_='info')
                            date_str = date_elem.get_text().strip() if date_elem else ""
                            
                            news_data.append({
                                'title': title,
                                'summary': summary,
                                'link': link,
                                'date': date_str,
                                'source': 'naver',
                                'search_term': term
                            })
                    except Exception as e:
                        continue
                
                time.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            
            return news_data
            
        except Exception as e:
            print(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ëž˜í•‘ ì˜¤ë¥˜: {e}")
            return []
    
    def scrape_google_news(self, ticker, company_name, days=7):
        """êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ ê¸°ì—… ê´€ë ¨ ë‰´ìŠ¤ ìŠ¤í¬ëž˜í•‘"""
        try:
            news_data = []
            
            # RSS í”¼ë“œë¥¼ í†µí•œ êµ¬ê¸€ ë‰´ìŠ¤ ìˆ˜ì§‘
            search_terms = [company_name, ticker.replace('.KS', '')]
            
            for term in search_terms:
                encoded_term = quote(term)
                rss_url = f"https://news.google.com/rss/search?q={encoded_term}&hl=ko&gl=KR&ceid=KR:ko"
                
                try:
                    feed = feedparser.parse(rss_url)
                    
                    for entry in feed.entries[:10]:  # ìƒìœ„ 10ê°œë§Œ
                        # ë‚ ì§œ íŒŒì‹±
                        pub_date = entry.published if hasattr(entry, 'published') else ""
                        
                        news_data.append({
                            'title': entry.title,
                            'summary': entry.summary if hasattr(entry, 'summary') else "",
                            'link': entry.link,
                            'date': pub_date,
                            'source': 'google_news',
                            'search_term': term
                        })
                except Exception as e:
                    print(f"RSS í”¼ë“œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
                
                time.sleep(1)
            
            return news_data
            
        except Exception as e:
            print(f"êµ¬ê¸€ ë‰´ìŠ¤ ìŠ¤í¬ëž˜í•‘ ì˜¤ë¥˜: {e}")
            return []
    
    def scrape_financial_news(self, ticker):
        """ê¸ˆìœµ ì „ë¬¸ ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            news_data = []
            
            # ì—¬ëŸ¬ ê¸ˆìœµ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ URL íŒ¨í„´
            financial_sites = [
                f"https://finance.naver.com/item/news.nhn?code={ticker.replace('.KS', '')}",
                # ì¶”ê°€ ê¸ˆìœµ ì‚¬ì´íŠ¸ë“¤...
            ]
            
            for site_url in financial_sites:
                try:
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    }
                    
                    response = requests.get(site_url, headers=headers)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ íŒŒì‹±
                    if 'finance.naver.com' in site_url:
                        news_items = soup.find_all('tr')
                        
                        for item in news_items[:15]:
                            try:
                                link_elem = item.find('a')
                                if link_elem and 'title' in link_elem.attrs:
                                    title = link_elem.get('title')
                                    link = "https://finance.naver.com" + link_elem.get('href')
                                    
                                    # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
                                    date_elem = item.find('td', class_='date')
                                    date_str = date_elem.get_text().strip() if date_elem else ""
                                    
                                    news_data.append({
                                        'title': title,
                                        'summary': "",
                                        'link': link,
                                        'date': date_str,
                                        'source': 'naver_finance',
                                        'search_term': ticker
                                    })
                            except Exception as e:
                                continue
                    
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"ê¸ˆìœµ ì‚¬ì´íŠ¸ ìŠ¤í¬ëž˜í•‘ ì˜¤ë¥˜: {e}")
                    continue
            
            return news_data
            
        except Exception as e:
            print(f"ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return []
    
    def analyze_sentiment_with_finbert(self, text):
        """FinBERTë¥¼ ì‚¬ìš©í•œ ê¸ˆìœµ ê°ì • ë¶„ì„"""
        try:
            if not self.sentiment_pipeline:
                return {"label": "NEUTRAL", "score": 0.5}
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (BERT ëª¨ë¸ì˜ í† í° ì œí•œ)
            if len(text) > 512:
                text = text[:512]
            
            result = self.sentiment_pipeline(text)[0]
            
            # FinBERT ê²°ê³¼ë¥¼ í‘œì¤€í™”
            label_mapping = {
                'positive': 'POSITIVE',
                'negative': 'NEGATIVE', 
                'neutral': 'NEUTRAL'
            }
            
            standard_label = label_mapping.get(result['label'].lower(), 'NEUTRAL')
            
            return {
                "label": standard_label,
                "score": result['score']
            }
            
        except Exception as e:
            print(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"label": "NEUTRAL", "score": 0.5}
    
    def process_news_data(self, news_data):
        """ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ ë° ê°ì • ë¶„ì„"""
        try:
            processed_news = []
            
            for news in news_data:
                # í…ìŠ¤íŠ¸ ì •ì œ
                title = news.get('title', '').strip()
                summary = news.get('summary', '').strip()
                
                # ì œëª©ê³¼ ìš”ì•½ì„ í•©ì¹œ í…ìŠ¤íŠ¸ë¡œ ê°ì • ë¶„ì„
                full_text = f"{title}. {summary}".strip()
                
                if len(full_text) > 10:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                    sentiment = self.analyze_sentiment_with_finbert(full_text)
                    
                    processed_news.append({
                        'title': title,
                        'summary': summary,
                        'link': news.get('link', ''),
                        'date': news.get('date', ''),
                        'source': news.get('source', ''),
                        'sentiment_label': sentiment['label'],
                        'sentiment_score': sentiment['score'],
                        'full_text': full_text
                    })
            
            return processed_news
            
        except Exception as e:
            print(f"ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return []
    
    def calculate_overall_sentiment(self, processed_news):
        """ì „ì²´ ë‰´ìŠ¤ì˜ ì¢…í•© ê°ì • ì ìˆ˜ ê³„ì‚°"""
        try:
            if not processed_news:
                return {
                    "overall_sentiment": "NEUTRAL",
                    "sentiment_score": 0.5,
                    "positive_count": 0,
                    "negative_count": 0,
                    "neutral_count": 0,
                    "total_news": 0
                }
            
            positive_count = 0
            negative_count = 0
            neutral_count = 0
            sentiment_scores = []
            
            for news in processed_news:
                label = news['sentiment_label']
                score = news['sentiment_score']
                
                if label == 'POSITIVE':
                    positive_count += 1
                    sentiment_scores.append(score)
                elif label == 'NEGATIVE':
                    negative_count += 1
                    sentiment_scores.append(-score)  # ë¶€ì •ì  ì ìˆ˜
                else:
                    neutral_count += 1
                    sentiment_scores.append(0)
            
            # ì „ì²´ ê°ì • ì ìˆ˜ ê³„ì‚°
            avg_score = np.mean(sentiment_scores) if sentiment_scores else 0
            
            # ì „ì²´ ê°ì • ë ˆì´ë¸” ê²°ì •
            if avg_score > 0.1:
                overall_sentiment = "POSITIVE"
            elif avg_score < -0.1:
                overall_sentiment = "NEGATIVE"
            else:
                overall_sentiment = "NEUTRAL"
            
            return {
                "overall_sentiment": overall_sentiment,
                "sentiment_score": avg_score,
                "positive_count": positive_count,
                "negative_count": negative_count,
                "neutral_count": neutral_count,
                "total_news": len(processed_news)
            }
            
        except Exception as e:
            print(f"ì¢…í•© ê°ì • ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {
                "overall_sentiment": "NEUTRAL",
                "sentiment_score": 0.0,
                "positive_count": 0,
                "negative_count": 0,
                "neutral_count": 0,
                "total_news": 0
            }
    
    def comprehensive_news_analysis(self, ticker, company_name=None):
        """ì¢…í•©ì ì¸ ë‰´ìŠ¤ ë¶„ì„ ìˆ˜í–‰"""
        print(f"=== {ticker} ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë¶„ì„ ì‹œìž‘ ===")
        
        # íšŒì‚¬ëª…ì´ ì—†ìœ¼ë©´ yfinanceì—ì„œ ê°€ì ¸ì˜¤ê¸°
        if not company_name:
            try:
                stock = yf.Ticker(ticker)
                company_name = stock.info.get('longName', ticker)
            except:
                company_name = ticker
        
        print(f"ë¶„ì„ ëŒ€ìƒ: {company_name} ({ticker})")
        
        all_news = []
        
        # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘
        print("ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        naver_news = self.scrape_naver_news(ticker, company_name)
        all_news.extend(naver_news)
        
        # 2. êµ¬ê¸€ ë‰´ìŠ¤ ìˆ˜ì§‘  
        print("êµ¬ê¸€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        google_news = self.scrape_google_news(ticker, company_name)
        all_news.extend(google_news)
        
        # 3. ê¸ˆìœµ ì „ë¬¸ ë‰´ìŠ¤ ìˆ˜ì§‘
        print("ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        financial_news = self.scrape_financial_news(ticker)
        all_news.extend(financial_news)
        
        print(f"ì´ {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # 4. ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ ë° ê°ì • ë¶„ì„
        print("ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì¤‘...")
        processed_news = self.process_news_data(all_news)
        
        # 5. ì¢…í•© ê°ì • ë¶„ì„
        sentiment_summary = self.calculate_overall_sentiment(processed_news)
        
        return {
            "ticker": ticker,
            "company_name": company_name,
            "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "raw_news": all_news,
            "processed_news": processed_news,
            "sentiment_summary": sentiment_summary
        }
    
    def format_news_report(self, analysis_data):
        """ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ ë³´ê³ ì„œë¡œ í¬ë§·íŒ…"""
        ticker = analysis_data.get('ticker', 'N/A')
        company_name = analysis_data.get('company_name', 'N/A')
        sentiment = analysis_data.get('sentiment_summary', {})
        processed_news = analysis_data.get('processed_news', [])
        
        report = f"""
=== {company_name} ({ticker}) ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë¶„ì„ ë³´ê³ ì„œ ===

## ë‰´ìŠ¤ ìˆ˜ì§‘ í˜„í™©
- ì´ ë‰´ìŠ¤ ê°œìˆ˜: {sentiment.get('total_news', 0)}ê°œ
- ê¸ì • ë‰´ìŠ¤: {sentiment.get('positive_count', 0)}ê°œ
- ë¶€ì • ë‰´ìŠ¤: {sentiment.get('negative_count', 0)}ê°œ  
- ì¤‘ë¦½ ë‰´ìŠ¤: {sentiment.get('neutral_count', 0)}ê°œ

## ì¢…í•© ê°ì • ë¶„ì„ (FinBERT ê¸°ë°˜)
- ì „ì²´ ê°ì •: {sentiment.get('overall_sentiment', 'N/A')}
- ê°ì • ì ìˆ˜: {sentiment.get('sentiment_score', 0):.3f}

## ì£¼ìš” ë‰´ìŠ¤ í—¤ë“œë¼ì¸ (ìµœê·¼ ìˆœ)
"""
        
        # ìƒìœ„ 10ê°œ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ì¶”ê°€
        for i, news in enumerate(processed_news[:10], 1):
            sentiment_emoji = {
                'POSITIVE': 'ðŸ“ˆ',
                'NEGATIVE': 'ðŸ“‰', 
                'NEUTRAL': 'ðŸ“Š'
            }.get(news['sentiment_label'], 'ðŸ“Š')
            
            report += f"{i}. {sentiment_emoji} {news['title'][:100]}...\n"
            if news['summary']:
                report += f"   ðŸ’¬ {news['summary'][:150]}...\n"
            report += f"   ðŸ”— {news['source']} | {news['date']}\n\n"
        
        # íˆ¬ìž ê´€ì ì—ì„œì˜ í•´ì„
        report += f"""
## íˆ¬ìž ê´€ì ì—ì„œì˜ í•´ì„
"""
        
        if sentiment.get('overall_sentiment') == 'POSITIVE':
            report += "- ðŸ“ˆ ì „ë°˜ì ìœ¼ë¡œ ê¸ì •ì ì¸ ë‰´ìŠ¤ê°€ ìš°ì„¸í•˜ì—¬ ì£¼ê°€ì— í˜¸ìž¬ë¡œ ìž‘ìš©í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n"
        elif sentiment.get('overall_sentiment') == 'NEGATIVE':
            report += "- ðŸ“‰ ë¶€ì •ì ì¸ ë‰´ìŠ¤ê°€ ë§Žì•„ ì£¼ê°€ì— ì•…ìž¬ë¡œ ìž‘ìš©í•  ê°€ëŠ¥ì„±ì„ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        else:
            report += "- ðŸ“Š ì¤‘ë¦½ì ì¸ ë‰´ìŠ¤ íë¦„ìœ¼ë¡œ ë‹¨ê¸°ì  ì£¼ê°€ ì˜í–¥ì€ ì œí•œì ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.\n"
        
        report += f"""
â€» ì´ ë¶„ì„ì€ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë°ì´í„°ì™€ FinBERT ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê²ƒì´ë©°, 
   íˆ¬ìž ê²°ì • ì‹œ ë‹¤ë¥¸ ìš”ì†Œë“¤ê³¼ í•¨ê»˜ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
"""
        
        return report


# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
def analyze_news(ticker, company_name=None):
    """ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = NewsAnalyzer()
    
    # ì¢…í•© ë‰´ìŠ¤ ë¶„ì„ ìˆ˜í–‰
    analysis_result = analyzer.comprehensive_news_analysis(ticker, company_name)
    
    # ë³´ê³ ì„œ í¬ë§·íŒ…
    report = analyzer.format_news_report(analysis_result)
    
    return report


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    test_ticker = "005930.KS"  # ì‚¼ì„±ì „ìž
    result = analyze_news(test_ticker)
    print(result) 