import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import pandas as pd
from newspaper import Article
from transformers import BertTokenizer, BertForSequenceClassification
import torch
from human_research import fetch_news, fetch_article_content, compute_sentiment_score, fetch_news_with_full_body_and_sentiment

class NewsAnalyzer:
    def __init__(self):
        try:
            self.finbert_tokenizer = BertTokenizer.from_pretrained("yiyanghkust/finbert-tone")
            self.finbert_model = BertForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")
            
            self.kobert_tokenizer = BertTokenizer.from_pretrained("snunlp/KR-FinBERT-SC")
            self.kobert_model = BertForSequenceClassification.from_pretrained("snunlp/KR-FinBERT-SC")
            self.kobert_model.eval()
            
            print("FinBERT + KoBERT ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"ë‰´ìŠ¤ ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")

            self.use_keyword_fallback = True
            self._init_keyword_system()
    
    def _init_keyword_system(self):
        """ë°±ì—…ìš© í‚¤ì›Œë“œ ê¸°ë°˜ ì‹œìŠ¤í…œ"""
        self.positive_keywords = [
            'ìƒìŠ¹', 'ê¸‰ë“±', 'ê°•ì„¸', 'í˜¸ì¬', 'ì„±ì¥', 'ì¦ê°€', 'ê°œì„ ', 'í™•ëŒ€', 'íˆ¬ì', 'ìˆ˜ìµ',
            'ê¸ì •', 'ê¸°ëŒ€', 'ì „ë§', 'ì¢‹ì€', 'ìš°ìˆ˜', 'ì„±ê³¼', 'ì‹¤ì ', 'ì´ìµ', 'ìˆ˜ì£¼', 'ê³„ì•½'
        ]
        self.negative_keywords = [
            'í•˜ë½', 'ê¸‰ë½', 'ì•½ì„¸', 'ì•…ì¬', 'ê°ì†Œ', 'ì¶•ì†Œ', 'ìš°ë ¤', 'ë¶€ì •', 'ìœ„í—˜', 'ì†ì‹¤',
            'ê±±ì •', 'ë¶ˆì•ˆ', 'ì €ì¡°', 'ë¶€ì§„', 'ì ì', 'ë¬¸ì œ', 'ìœ„ê¸°', 'íƒ€ê²©', 'ì¶©ê²©', 'ë¶€ë‹´'
        ]
    
    def analyze_sentiment_with_finbert(self, text):
        """FinBERTë¥¼ ì‚¬ìš©í•œ ì˜ë¬¸ ë‰´ìŠ¤ ê°ì • ë¶„ì„"""
        try:
            text = self.clean_text(text)
            tokens = self.finbert_tokenizer.encode(text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')
            with torch.no_grad():
                output = self.finbert_model(tokens)
            logits = output.logits
            probs = torch.softmax(logits, dim=1).numpy()[0]
            sentiment_score = probs[2] - probs[0]  # positive - negative
            return round(float(sentiment_score), 4)
        except Exception as e:
            print(f"FinBERT ë¶„ì„ ì˜¤ë¥˜: {e}")
            return 0.0
    
    def analyze_sentiment_with_clova(self, text):
        """Clova AIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ê°ì • ë¶„ì„"""
        try:
            return response.json()["sentiment"]
        except Exception as e:
            print(f"Clova AI ë¶„ì„ ì˜¤ë¥˜: {e}")


    def analyze_sentiment_with_kobert(self, text):
        """KoBERTë¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´ ë‰´ìŠ¤ ê°ì • ë¶„ì„"""
        try:
            inputs = self.kobert_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
            with torch.no_grad():
                outputs = self.kobert_model(**inputs)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1).squeeze()
            score = -1 * probs[0].item() + 0 * probs[1].item() + 1 * probs[2].item()
            return round(score, 4)
        except Exception as e:
            print(f"KoBERT ë¶„ì„ ì˜¤ë¥˜: {e}")
            return 0.0
    
    def analyze_sentiment_with_keywords(self, text):
        """ë°±ì—…ìš© í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„"""
        if not text:
            return 0.0
        
        text = text.lower()
        positive_count = sum(1 for keyword in self.positive_keywords if keyword in text)
        negative_count = sum(1 for keyword in self.negative_keywords if keyword in text)
        
        total_keywords = positive_count + negative_count
        if total_keywords == 0:
            return 0.0
        
        sentiment_score = (positive_count - negative_count) / total_keywords
        return round(sentiment_score, 4)
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""
        text = text.lower()
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'[^a-zA-Z0-9.,!?\'\"%$()-]', ' ', text)
        text = text.strip()
        return text
    
    def is_korean_ticker(self, ticker):
        """í•œêµ­ ì£¼ì‹ ì½”ë“œ ì—¬ë¶€ í™•ì¸ (6ìë¦¬ ìˆ«ì)"""
        return bool(re.match(r'^\d{6}$', ticker))
    
    def fetch_korean_news(self, ticker, max_pages=5):
        """í•œêµ­ ì£¼ì‹ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° KoBERT ê°ì • ë¶„ì„"""
        try:
            # human_researchì˜ fetch_news í•¨ìˆ˜ëŠ” stock_codeë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ìŒ
            news_list = fetch_news(ticker, max_pages=max_pages)
            analyzed_news = []
            
            for news in news_list[:10]:  # ìµœëŒ€ 10ê°œ
                try:
                    content = fetch_article_content(news["ë§í¬"])
                    if content and content != "None":
                        # í´ë¡œë°” ê°ì • ë¶„ì„ ì‚¬ìš© (human_researchì˜ compute_sentiment_score í•¨ìˆ˜ ì‚¬ìš©)
                        sentiment_score = compute_sentiment_score(content)
                        analyzed_news.append({
                            "title": news["ì œëª©"],
                            "content": content[:500],  # ìš”ì•½
                            "date": news["ë‚ ì§œ"],
                            "sentiment": sentiment_score
                        })
                except Exception as e:
                    print(f"í•œêµ­ ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            return analyzed_news
            
        except Exception as e:
            print(f"í•œêµ­ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return []
    
    def fetch_english_news(self, ticker, days=3, max_results=20):
        """ì˜ë¬¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° FinBERT ê°ì • ë¶„ì„"""
        try:
            # NewsAPIë¥¼ ì‚¬ìš©í•œ ì˜ë¬¸ ë‰´ìŠ¤ ìˆ˜ì§‘
            url = "https://newsapi.org/v2/everything"
            end_date = datetime.utcnow()
            start_date = end_date - timedelta(days=days)
            from_date_str = start_date.strftime("%Y-%m-%dT%H:%M:%SZ")
            
            params = {
                "q": ticker,
                "from": from_date_str,
                "language": "en",
                "sortBy": "publishedAt",
                "pageSize": max_results,
                "apiKey": "5a9d9ae903084a349d94fdff1fb3da6a"
            }
            
            response = requests.get(url, params=params)
            data = response.json()
            
            if data.get("status") != "ok":
                print(f"NewsAPI ì˜¤ë¥˜: {data.get('message')}")
                return []
            
            articles = data.get("articles", [])
            analyzed_news = []
            
            for article in articles[:10]:  # ìµœëŒ€ 10ê°œ
                try:
                    title = article.get("title", "")
                    description = article.get("description", "")
                    url_link = article.get("url", "")
                    published_at = article.get("publishedAt", "")
                    
                    # ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œë„
                    full_text = description  # ê¸°ë³¸ê°’
                    try:
                        news_article = Article(url_link)
                        news_article.download()
                        news_article.parse()
                        if news_article.text:
                            full_text = news_article.text[:1000]  # ìµœëŒ€ 1000ì
                    except:
                        pass
                    
                    sentiment_score = self.analyze_sentiment_with_finbert(full_text)
                    
                    analyzed_news.append({
                        "title": title,
                        "content": full_text[:500],  # ìš”ì•½
                        "date": published_at[:10] if published_at else "",
                        "sentiment": sentiment_score
                    })
                    
                except Exception as e:
                    print(f"ì˜ë¬¸ ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            return analyzed_news
            
        except Exception as e:
            print(f"ì˜ë¬¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return []
    
    def analyze_news(self, ticker):
        """ì¢…í•© ë‰´ìŠ¤ ë¶„ì„"""
        try:
            print(f"=== {ticker} ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘ ===")
            
            if self.is_korean_ticker(ticker):
                print(f"ë¶„ì„ ëŒ€ìƒ: í•œêµ­ ì£¼ì‹ ({ticker})")
                korean_news = self.fetch_korean_news(ticker)
                english_news = []  # í•œêµ­ ì£¼ì‹ì€ ì˜ë¬¸ ë‰´ìŠ¤ ì œì™¸
                all_news = korean_news
            else:
                # ì˜ë¬¸ í‹°ì»¤ì˜ ê²½ìš° íšŒì‚¬ëª… ì¶”ì¶œ
                company_names = {
                    'AAPL': 'Apple Inc.',
                    'GOOGL': 'Alphabet Inc.',
                    'MSFT': 'Microsoft Corporation',
                    'AMZN': 'Amazon.com Inc.',
                    'TSLA': 'Tesla, Inc.',
                    'NVDA': 'NVIDIA Corporation',
                    'META': 'Meta Platforms Inc.',
                    'NFLX': 'Netflix Inc.'
                }
                company_name = company_names.get(ticker, ticker)
                print(f"ë¶„ì„ ëŒ€ìƒ: {company_name} ({ticker})")
                
                korean_news = []  # ì˜ë¬¸ ì£¼ì‹ì€ í•œêµ­ ë‰´ìŠ¤ ì œì™¸
                english_news = self.fetch_english_news(ticker)
                all_news = english_news
            
            print(f"ì´ {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            
            if not all_news:
                return self._generate_no_news_report(ticker)
            
            # ê°ì • ë¶„ì„ í†µê³„
            sentiments = [news['sentiment'] for news in all_news if news['sentiment'] is not None]
            avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0
            positive_count = len([s for s in sentiments if s > 0.1])
            negative_count = len([s for s in sentiments if s < -0.1])
            neutral_count = len(sentiments) - positive_count - negative_count
            
            print("ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì¤‘...")
            
            # ì£¼ìš” ë‰´ìŠ¤ ìš”ì•½ (ê°ì • ì ìˆ˜ ìƒìœ„/í•˜ìœ„)
            sorted_news = sorted(all_news, key=lambda x: abs(x['sentiment']), reverse=True)
            top_news = sorted_news[:5]
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = self._generate_news_report(ticker, all_news, avg_sentiment, positive_count, negative_count, neutral_count, top_news)
            
            return report
            
        except Exception as e:
            print(f"ë‰´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return f"""
## ğŸ“° **{ticker} ë‰´ìŠ¤ ë¶„ì„**

âŒ **ë¶„ì„ ì˜¤ë¥˜**: {str(e)}

ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.
            """
    
    def _generate_no_news_report(self, ticker):
        """ë‰´ìŠ¤ê°€ ì—†ì„ ë•Œ ë¦¬í¬íŠ¸"""
        return f"""
## ğŸ“° **{ticker} ë‰´ìŠ¤ ë¶„ì„**

ğŸ“Š **ë¶„ì„ ê²°ê³¼**: ìµœê·¼ ê´€ë ¨ ë‰´ìŠ¤ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

ğŸ’¡ **ì°¸ê³ ì‚¬í•­**: 
- ë‰´ìŠ¤ ë°ì´í„°ê°€ ì¼ì‹œì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì¢…ëª© ì½”ë“œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”
"""
    
    def _generate_news_report(self, ticker, all_news, avg_sentiment, positive_count, negative_count, neutral_count, top_news):
        """ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        # ê°ì • í•´ì„
        if avg_sentiment > 0.2:
            sentiment_desc = "ğŸ“ˆ **ë§¤ìš° ê¸ì •ì **"
            sentiment_icon = "ğŸŸ¢"
        elif avg_sentiment > 0.05:
            sentiment_desc = "ğŸ“Š **ê¸ì •ì **"
            sentiment_icon = "ğŸ”µ"
        elif avg_sentiment > -0.05:
            sentiment_desc = "âš–ï¸ **ì¤‘ë¦½ì **"
            sentiment_icon = "ğŸŸ¡"
        elif avg_sentiment > -0.2:
            sentiment_desc = "ğŸ“‰ **ë¶€ì •ì **"
            sentiment_icon = "ğŸŸ "
        else:
            sentiment_desc = "âš ï¸ **ë§¤ìš° ë¶€ì •ì **"
            sentiment_icon = "ğŸ”´"
        
        # ì£¼ìš” ë‰´ìŠ¤ ìš”ì•½
        news_summary = ""
        for i, news in enumerate(top_news[:3], 1):
            sentiment_emoji = "ğŸ“ˆ" if news['sentiment'] > 0 else "ğŸ“‰" if news['sentiment'] < 0 else "âš–ï¸"
            news_summary += f"""
**{i}. {news['title'][:50]}...**
{sentiment_emoji} ê°ì •ì ìˆ˜: {news['sentiment']:+.3f}
ğŸ“… {news['date']}
---
"""
        
        model_type = "KoBERT" if self.is_korean_ticker(ticker) else "FinBERT"
        
        report = f"""
## ğŸ“° **{ticker} ë‰´ìŠ¤ ë¶„ì„**

### {sentiment_icon} **ì¢…í•© ê°ì • ë¶„ì„ ({model_type} ê¸°ë°˜)**
- **ì „ì²´ í‰ê· **: {avg_sentiment:+.3f} {sentiment_desc}
- **ê¸ì • ë‰´ìŠ¤**: {positive_count}ê°œ ({positive_count/len(all_news)*100:.1f}%)
- **ì¤‘ë¦½ ë‰´ìŠ¤**: {neutral_count}ê°œ ({neutral_count/len(all_news)*100:.1f}%)  
- **ë¶€ì • ë‰´ìŠ¤**: {negative_count}ê°œ ({negative_count/len(all_news)*100:.1f}%)

### ğŸ“‹ **ì£¼ìš” ë‰´ìŠ¤ í—¤ë“œë¼ì¸**
{news_summary}

### ğŸ’¡ **ë¶„ì„ ìš”ì•½**
- **ìˆ˜ì§‘ ë‰´ìŠ¤**: ì´ {len(all_news)}ê±´
- **ë¶„ì„ ëª¨ë¸**: {model_type} (íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜)
- **ì‹œì¥ ì‹¬ë¦¬**: {sentiment_desc}

â€» ì´ ë¶„ì„ì€ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë°ì´í„°ì™€ {model_type} ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê²ƒì´ë©°, 
íˆ¬ì ì˜ì‚¬ê²°ì • ì‹œ ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
"""
        return report

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
news_analyzer = NewsAnalyzer()

def run_news_analysis(ticker):
    """ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜ (backend.pyì™€ì˜ í˜¸í™˜ì„±)"""
    return news_analyzer.analyze_news(ticker)

def analyze_news(ticker, company_name=None):
    """ì´ì „ ë²„ì „ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    return news_analyzer.analyze_news(ticker) 